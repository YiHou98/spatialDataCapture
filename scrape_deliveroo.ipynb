{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/sds2020/bin/python: can't open file 'pip': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one type of restaurants from one postcode\n",
    "def get_deliveroo_restaurants(categery,postcode):\n",
    "    ##add postcode&categery into url\n",
    "    url = \"https://deliveroo.co.uk/restaurants/london/camden?postcode=\"+ postcode +\"&collection=\" + categery\n",
    "    cookies = {'cookie': 'Bearer eyJhbGciOiJFUzI1NiIsImprdSI6Imh0dHBzOi8vZGVsaXZlcm9vLmNvLnVrL2lkZW50aXR5LWtleXMvMS5qd2sifQ.eyJleHAiOjE2MTE1MDQ3NTEsImN1c3QiOjQyMDEwNTkzLCJkcm5faWQiOiIwNjhiZGU3Mi01ZThjLTQ3ZTUtYmU2Mi01ZWI1NTc5NTBhNWIiLCJzZXNzIjoid2ViLDhlZjZkZmVhYjdlZTQ3MDY4NTFkMmZlZmU3NDFkYjc3In0.QNnAFk67xFNZI64BHdP2iNPXcxvExJDEVj3PCGs2VTEdViFqyD539-VeppsR7TLQ5SGQZ5sJdroNO88CQVaBj'}\n",
    "    resp = requests.get(url, cookies=cookies)\n",
    "    html = resp.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    order_history = soup.find(\"ul\", {\"class\": \"HomeFeedGrid-06181a3851a8467d\"})\n",
    "    order_entry = order_history.findAll(\"li\")\n",
    "    order_entry= order_entry[1:]\n",
    "    aria_label = []\n",
    "    url1= 'https://deliveroo.co.uk'\n",
    "    order_url = []\n",
    "    aria_label = []\n",
    "    for order in order_entry:\n",
    "        if order is not None:\n",
    "            items = []\n",
    "            label = order.find(\"a\")\n",
    "            if label is not None:\n",
    "                order_url.append(url1  + label.get('href'))\n",
    "                aria_label.append(label.get('aria-label'))\n",
    "    name = []\n",
    "    rate = []\n",
    "    description = []\n",
    "    address = []\n",
    "    photo = []\n",
    "    ## get name&rate\n",
    "    for detail in order_url:\n",
    "        resp = requests.get(detail, cookies=cookies)\n",
    "        html = resp.content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        if soup is not None:\n",
    "            name1 =soup.find(\"h1\", {\"class\": \"ccl-2a4b5924e2237093 ccl-21bead492ce4ada2 ccl-9ff886da4b0592ae ccl-c9da0519c26dc749\"})\n",
    "            if name1 is not None:\n",
    "                name.append(name1.getText().strip())\n",
    "            else:\n",
    "                name.append('none')\n",
    "            rate1 = soup.find(\"span\", {\"class\": \"ccl-19882374e640f487 ccl-417df52a76832172 ccl-b308a2db3758e3e5\"})\n",
    "            if rate1 is not None:\n",
    "                rate.append(rate1.getText().strip())\n",
    "            else:\n",
    "                rate.append('none')\n",
    "    ## get description      \n",
    "    for detail in order_url:\n",
    "        resp = requests.get(detail, cookies=cookies)\n",
    "        html = resp.content\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        if soup is not None:\n",
    "            charac = soup.findAll(\"li\",{\"class\": \"orderweb__b5699496\"})\n",
    "            character = []\n",
    "            for i in charac:\n",
    "                if i is not None:\n",
    "                    character.append(i.getText().strip())\n",
    "        description.append(character)\n",
    "    ##get address from description\n",
    "    for i in description:\n",
    "        if i == []:\n",
    "            address.append('none')\n",
    "        else:\n",
    "            address.append(i[-2])\n",
    "    \n",
    "    \n",
    "    ## get photo_url,this needs a long time to run,and may have incomplete records\n",
    "    ## the webdriver is not compatible with virtualbox, you can run it on Anaconda or other tools\n",
    "    \n",
    "    #from selenium import webdriver\n",
    "    #driver = webdriver.Chrome(r'/Users/houyi/Documents/CASA/modules/i2p/chromedriver')\n",
    "    #link_regex = re.compile('(https://+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)  \n",
    "    #for detail in order_url:\n",
    "    #    driver.get(detail)\n",
    "    #    html = driver.page_source\n",
    "    #    soup = BeautifulSoup(html,'html.parser')\n",
    "    #    if soup is not None:\n",
    "    #        char = soup.findAll('div', {'class':'ccl-45bd106b75353ec9'})\n",
    "    #        for i in char:\n",
    "    #            if i is not None:\n",
    "    #                x = i.get('style')\n",
    "    #                if x is not None:\n",
    "    #                    if re.findall(link_regex, x)==[]:\n",
    "    #                        photo.append('none')\n",
    "    #                        break\n",
    "    #                    else:\n",
    "    #                        photo.append(re.findall(link_regex, x)[0][0])\n",
    "    #                        break\n",
    "    #            else:\n",
    "    #                photo.append('none')\n",
    "    data = {'name':name, 'rate':rate, 'address':address}\n",
    "    df = pd.DataFrame(data=data)\n",
    "    df.to_csv(categery+postcode+'.csv')\n",
    "    \n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##example\n",
    "##the categery includes pizza, burgers....You can find them on deliveroo website,please take care that you should input lowercase\n",
    "##healthy, but not Healthy\n",
    "get_deliveroo_restaurants('healthy','N1C4BN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
